{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5PmEpAk1nypU0sxeXnauu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/848498/WEB-SCRPING-ASSIGNMENT-2/blob/main/web_scraping_assignment_2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "url = \"https://www.shine.com\"\n",
        "response = requests.get(url)\n",
        "job_title = \"Data Analyst\"\n",
        "location = \"Bangalore\"\n",
        "search_url = f\"{url}/job-search/{job_title}-jobs-in-{location}\"\n",
        "response = requests.get(search_url)\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "job_results = soup.find_all(\"li\", class_=\"search_listing\")\n",
        "data = []\n",
        "for job in job_results[:10]:\n",
        "  title = job.find(\"h2\").text.strip()\n",
        "  company = job.find(\"span\", class_=\"company_name\").text.strip()\n",
        "  location = job.find(\"span\", class_=\"location\").text.strip()\n",
        "  experience = job.find(\"span\", class_=\"exp\").text.strip()\n",
        "\n",
        "  data.append({\n",
        "  \"Job Title\": title,\n",
        "  \"Company Name\": company,\n",
        "  \"Job Location\": location,\n",
        "  \"Experience Required\": experience\n",
        "  })\n",
        "df = pd.DataFrame(data)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7W38w9P10vIR",
        "outputId": "8892a7ec-c141-46ef-aa33-717907a23ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "url = \"https://www.shine.com\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "search_form = soup.find(\"form\", {\"id\": \"searchForm\"})\n",
        "job_title_field = search_form.find(\"input\", {\"id\": \"txt_search\"})\n",
        "location_field = search_form.find(\"input\", {\"id\": \"txt_location\"})\n",
        "\n",
        "job_title_field[\"value\"] = \"Data Scientist\"\n",
        "location_field[\"value\"] = \"Bangalore\"\n",
        "search_button = search_form.find(\"button\", {\"type\": \"submit\"})\n",
        "search_url = url + search_button[\"formaction\"]\n",
        "response = requests.post(search_url, data=search_form.serialize())\n",
        "job_results = soup.find_all(\"div\", {\"class\": \"search_listing\"})\n",
        "job_data = []\n",
        "  title = job.find(\"a\", {\"class\": \"job_title\"}).text.strip()\n",
        "  location = job.find(\"span\", {\"class\": \"job_location\"}).text.strip()\n",
        "  company = job.find(\"a\", {\"class\": \"company_name\"}).text.strip()\n",
        "  job_data.append({\"Job Title\": title, \"Location\": location, \"Company\": company})\n",
        "df = pd.DataFrame(job_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "BGl3-qjzIfj1",
        "outputId": "7abaed5a-82fb-4431-a5f0-78ba49ccb471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-7ecc47685d9d>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    title = job.find(\"a\", {\"class\": \"job_title\"}).text.strip()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "url = \"https://www.shine.com/\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "search_input = soup.find(\"input\", {\"id\": \"txt_search\"})\n",
        "search_input[\"value\"] = \"Data Scientist\"\n",
        "search_button = soup.find(\"button\", {\"id\": \"btn_search\"})\n",
        "response = requests.post(url, data={\"txt_search\": \"Data Scientist\"})\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "location_filter = soup.find(\"input\", {\"id\": \"chk_location_1\"})\n",
        "location_filter[\"checked\"] = True\n",
        "\n",
        "salary_filter = soup.find(\"input\", {\"id\": \"chk_salary_1\"})\n",
        "salary_filter[\"checked\"] = True\n",
        "job_results = soup.find_all(\"div\", {\"class\": \"search_listing\"})\n",
        "data = []\n",
        "\n",
        "for job in job_results[:10]:\n",
        "  job_title = job.find(\"h3\").text.strip()\n",
        "  job_location = job.find(\"span\", {\"class\": \"location\"}).text.strip()\n",
        "  company_name = job.find(\"span\", {\"class\": \"company_name\"}).text.strip()\n",
        "  experience_required = job.find(\"span\", {\"class\": \"exp\"}).text.strip()\n",
        "\n",
        "  data.append({\n",
        "  \"Job Title\": job_title,\n",
        "  \"Job Location\": job_location,\n",
        "  \"Company Name\": company_name,\n",
        "  \"Experience Required\": experience_required\n",
        "  })\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "CGQseq7RLoGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://www.flipkart.com/\"\n",
        "search_query = \"sunglasses\"\n",
        "max_listings = 100\n",
        "scraped_data = []\n",
        "\n",
        "while len(scraped_data) < max_listings:\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "  # Find the search field and enter the search query\n",
        "  search_field = soup.find(\"input\", attrs={\"title\": \"Search for products, brands and more\"})\n",
        "  search_field[\"value\"] = search_query\n",
        "\n",
        "  # Click the search icon\n",
        "  search_icon = soup.find(\"button\", attrs={\"type\": \"submit\"})\n",
        "  response = requests.post(url, data=search_icon.form)\n",
        "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "  # Find the container holding the listings\n",
        "  listings_container = soup.find(\"div\", attrs={\"class\": \"_1AtVbE\"})\n",
        "\n",
        "  # Extract the required attributes from each listing\n",
        "  for listing in listings_container.find_all(\"div\", attrs={\"class\": \"_2kHMtA\"}):\n",
        "  brand = listing.find(\"div\", attrs={\"class\": \"_2WkVRV\"}).text\n",
        "  description = listing.find(\"a\", attrs={\"class\": \"IRpwTa\"}).text\n",
        "  price = listing.find(\"div\", attrs={\"class\": \"_30jeq3 _1_WHN1\"}).text\n",
        "\n",
        "  scraped_data.append({\"Brand\": brand, \"ProductDescription\": description, \"Price\": price})\n",
        "\n",
        "  if len(scraped_data) == max_listings:\n",
        "\n",
        "  # Find the \"Next\" button and navigate to the next page\n",
        "  next_button = soup.find(\"a\", attrs={\"class\": \"_1LKTO3\"})\n",
        "  if next_button:\n",
        "  url = \"https://www.flipkart.com\" + next_button[\"href\"]\n",
        "  else:\n",
        "  break\n",
        "\n",
        "# Print the scraped data\n",
        "for data in scraped_data:\n",
        "  print(data)"
      ],
      "metadata": {
        "id": "xRnsVG5yHLyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Install necessary libraries\n",
        "\n",
        "# Step 2: Send GET request and retrieve HTML content\n",
        "url = \"https://www.motor1.com/\"\n",
        "response = requests.get(url)\n",
        "html_content = response.content\n",
        "\n",
        "# Step 3: Parse HTML content and extract search bar element\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "search_bar = soup.find('input', {'name': 'q'})\n",
        "\n",
        "# Step 4: Simulate typing and submit form\n",
        "\n",
        "# Step 5: Send GET request to search result page\n",
        "search_url = \"https://www.motor1.com/search/?q=50+most+expensive+cars\"\n",
        "search_response = requests.get(search_url)\n",
        "search_html_content = search_response.content\n",
        "\n",
        "# Step 6: Parse search result page and locate list of cars\n",
        "\n",
        "# Step 7: Extract car name and price\n",
        "\n",
        "# Step 8: Create dataframe"
      ],
      "metadata": {
        "id": "hh1qRYV_9WH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "url = \"https://www.jagaranjosh.com/\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response. content, \"html.parser\")\n",
        "gk_link = soup.find(\"a\", text=\"GK\")[\"href\"]\n",
        "gk_url = url + gk_link\n",
        "gk_response = requests.get(gk_url)\n",
        "gk_soup = BeautifulSoup(gk_response.content,\"html.parser\")\n",
        "pm_link = gk_soup.find(\"a\", text=\"List of all Prime Ministers Of India\")[\"href\"]\n",
        "pm_url = url + pm_link\n",
        "pm_response = requests.get(pm_url)\n",
        "pm_soup = BeautifulSoup(pm_response.content,\"html.parser\")\n",
        "table = pm_soup.find(\"table\")\n",
        "data = []\n",
        "for row in table.find_all(\"tr\"):\n",
        " cols = row.find_all(\"td\")\n",
        "if len(cols) ==4:\n",
        "  name = cols[0].text.strip()\n",
        "  born_dead = cols[1].text.strip()\n",
        "  term_of_office = cols[2].text.strip()\n",
        "  remarks = cols[3].text.strip()\n",
        "  data.append([name,born_dead,term_of_office,remarks])\n",
        "  df = pd.DataFrame(data, colums=[\"Name\", \"Born-Dead\", \"Term Of Office\", \"Remarks\"])\n",
        "  print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "R21DFihtRakE",
        "outputId": "cf49b00c-2678-4848-9fc4-d44a3aada61d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-abd14d7eef67>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    gk_link = soup.find(\"a\", text=\"GK\")[\"href\"\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '[' was never closed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.utils import text\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "url = \"https://www.azquotes.com/\"\n",
        "response = requests.get(url)\n",
        "html_content = response.content\n",
        "soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "top_quotes_link = soup.find (\"a\", text= topquotes\")[\"href\"])\n",
        "top_quotes_url = url + top_quotes_link\n",
        "                            response = requests.get(top_quotes_url)\n",
        "                            top_quotes_html = response.content\n",
        "                            top_quotes_soup = BeautifulSoup(top_quotes_html, \"html.parser\")\n",
        "                            quote_container = top_quotes_soup.find(\"div\", class_=\"list-quotes\")\n",
        "                            quote = quote_container.find_all(\"div\", class_=\"warp-blockquote\")\n",
        "                            quote_text = quote.find(\"a\", class_=\"title\").text\n",
        "                            author = quote.find(\"a\", class_=\"author\").text\n",
        "                            quote_type = quote.find(\"div\", class_=\"quote-tags\").text\n",
        "                             print(\"Quote:\", quote_text)\n",
        "                             print(\"Author:\", author)\n",
        "                             print(\"Type of Quote:\", quote_type)\n",
        "                             print()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "KggIpEIxgFVz",
        "outputId": "996bca4c-dcb0-4645-cb3a-1970dccd5b2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-cb92a26ea19d>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    top_quotes_link = soup.find (\"a\", text= topquotes\")[\"href\"])\u001b[0m\n\u001b[0m                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 8)\n"
          ]
        }
      ]
    }
  ]
}